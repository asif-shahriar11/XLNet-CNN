{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9200521,"sourceType":"datasetVersion","datasetId":5562432}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install numpy\n# !pip install pandas\n# !pip install scikit-learn\n# !pip install torch\n# !pip install transformers\n\nfrom transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, hamming_loss, roc_auc_score, average_precision_score\nfrom collections import defaultdict\nfrom torch.cuda.amp import autocast, GradScaler\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport time\n## Hyperparameters\nMAX_LEN = 512\nTRAIN_BATCH_SIZE = 8\nVALID_BATCH_SIZE = 8\nTEST_BATCH_SIZE = 8\nEPOCHS = 15\nLEARNING_RATE = 1e-05\nTHRESHOLD = 0.5 # threshold for the sigmoid\n## Dataset Class\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, df, tokenizer, max_len, target_list):\n        self.tokenizer = tokenizer\n        self.df = df\n        self.title = list(df['File Contents'])\n        self.targets = self.df[target_list].values\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.title)\n\n    def __getitem__(self, index):\n        title = str(self.title[index])\n        title = \" \".join(title.split())\n        inputs = self.tokenizer.encode_plus(\n            title,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_token_type_ids=True,\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': inputs['input_ids'].flatten(),\n            'attention_mask': inputs['attention_mask'].flatten(),\n            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n            'targets': torch.FloatTensor(self.targets[index]),\n            'title': title\n        }\n## Data\ntrain_file = '/kaggle/input/ohsumed-2/train.csv'\nval_file = '/kaggle/input/ohsumed-2/val.csv'\ntest_file = '/kaggle/input/ohsumed-2/test.csv'\ntrain_df = pd.read_csv(train_file)\nval_df = pd.read_csv(val_file)\ntest_df = pd.read_csv(test_file)\n\ntarget_list = list(train_df.columns[1:])\n## Tokenizer\ntokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\ntrain_dataset = CustomDataset(train_df, tokenizer, MAX_LEN, target_list)\nvalid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN, target_list)\ntest_dataset = CustomDataset(test_df, tokenizer, MAX_LEN, target_list)\n\nprint(train_dataset[0])\n## Data Loader\ntrain_data_loader = torch.utils.data.DataLoader(train_dataset,\n    batch_size=TRAIN_BATCH_SIZE,\n    shuffle=True,\n    num_workers=0\n)\n\nval_data_loader = torch.utils.data.DataLoader(valid_dataset,\n    batch_size=VALID_BATCH_SIZE,\n    shuffle=False,\n    num_workers=0\n)\n\ntest_data_loader = torch.utils.data.DataLoader(test_dataset,\n    batch_size=TEST_BATCH_SIZE,\n    shuffle=False,\n    num_workers=0\n)\n## Device\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndevice\n## Model\nclass XLNETBase(nn.Module):\n    def __init__(self, num_classes):\n        super(XLNETBase, self).__init__()\n        self.xlnet = XLNetModel.from_pretrained('xlnet-base-cased')\n        self.drop = nn.Dropout(0.3)\n        self.out = nn.Linear(768, num_classes)  # Assuming 23 classes for classification\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        # Get the full output\n        outputs = self.xlnet(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n\n        # The last hidden state is the first element of the output\n        last_hidden_state = outputs[0]\n\n        # Pooling operation if needed, for example, using the [CLS] token's embedding\n        # Assuming [CLS] is the first token, similar to BERT. Adjust as needed.\n        pooled_output = last_hidden_state[:, 0]\n\n        output = self.drop(pooled_output)\n        return self.out(output)\n## Setting the model\nmodel = XLNETBase(num_classes=len(target_list))\nmodel.to(device)\n## Loss & Optimizer\ndef loss_fn(outputs, targets):\n    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n\n# define the optimizer\noptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n## Training function\ndef train_model(training_loader, model, optimizer, accumulation_steps=4):\n    losses = []\n    correct_predictions = 0\n    num_samples = 0\n    total_batches = len(training_loader)\n\n    # Set model to training mode (activate dropout, batch norm)\n    model.train()\n\n    # Mixed precision\n    scaler = GradScaler()\n\n    for batch_idx, data in enumerate(training_loader):\n        ids = data['input_ids'].to(device, dtype=torch.long)\n        mask = data['attention_mask'].to(device, dtype=torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n        targets = data['targets'].to(device, dtype=torch.float)\n\n        # Forward pass with mixed precision\n        with autocast():\n            outputs = model(ids, mask, token_type_ids)  # (batch, predict) = (8, 8)\n            loss = loss_fn(outputs, targets)\n            losses.append(loss.item())\n\n        # Training accuracy, apply sigmoid, round (apply threshold 0.5)\n        outputs = torch.sigmoid(outputs).cpu().detach().numpy().round()\n        targets = targets.cpu().detach().numpy()\n        correct_predictions += np.sum(outputs == targets)\n        num_samples += targets.size  # Total number of elements in the 2D array\n\n        # Backward pass with gradient accumulation\n        loss = loss / accumulation_steps  # Normalize loss to account for accumulation\n        scaler.scale(loss).backward()\n\n        # Gradient clipping\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        # Step optimizer every accumulation_steps\n        if (batch_idx + 1) % accumulation_steps == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n\n        # Clear GPU cache\n        torch.cuda.empty_cache()\n\n    # Perform the final optimizer step if not done already\n    if (batch_idx + 1) % accumulation_steps != 0:\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad()\n\n    # Returning: trained model, model accuracy, mean loss\n    return model, float(correct_predictions) / num_samples, np.mean(losses)\n## Evaluator Function\ndef eval_model(validation_loader, model, threshold=0.5, target_list=None):\n    model.eval()\n    final_targets = []\n    final_outputs = []\n    final_probs = []\n    losses = []\n\n    # Mixed precision\n    scaler = torch.cuda.amp.GradScaler()\n\n    with torch.no_grad():\n        for data in validation_loader:\n            ids = data['input_ids'].to(device, dtype=torch.long)\n            mask = data['attention_mask'].to(device, dtype=torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n            targets = data['targets'].to(device, dtype=torch.float)\n\n            # Mixed precision forward pass\n            with torch.cuda.amp.autocast():\n                outputs = model(ids, mask, token_type_ids)\n                loss = loss_fn(outputs, targets)\n                losses.append(loss.item())\n\n                probs = torch.sigmoid(outputs).cpu().detach().numpy()\n                targets = targets.cpu().detach().numpy()\n                final_outputs.extend(probs >= threshold)\n                final_probs.extend(probs)\n                final_targets.extend(targets)\n\n            # Clear GPU cache\n            torch.cuda.empty_cache()\n\n    final_outputs = np.array(final_outputs) >= threshold\n    final_probs = np.array(final_probs)\n    final_targets = np.array(final_targets)\n\n    # Calculating metrics\n    acc = accuracy_score(final_targets, final_outputs)\n    f1 = f1_score(final_targets, final_outputs, average='weighted')  # Consider using 'macro' or 'weighted' based on your problem\n    precision = precision_score(final_targets, final_outputs, average='weighted')\n    recall = recall_score(final_targets, final_outputs, average='weighted')\n    hamming = hamming_loss(final_targets, final_outputs)\n\n    auc_roc = roc_auc_score(final_targets, final_probs, average='weighted', multi_class='ovr')\n    aupr = average_precision_score(final_targets, final_probs, average='weighted')\n\n    average_loss = np.mean(losses)\n\n    print(f\"Accuracy: {acc}\")\n    print(f\"F1 Score: {f1}\")\n    print(f\"Precision: {precision}\")\n    print(f\"Recall: {recall}\")\n    print(f\"Hamming Loss: {hamming}\")\n    print(f\"Average Loss: {average_loss}\")\n    print(f\"AUC-ROC: {auc_roc}\")\n    print(f\"AUPR: {aupr}\")\n    # Detailed classification report\n#     if target_list:\n#         print(\"\\nClassification Report:\\n\", classification_report(final_targets, final_outputs, target_names=target_list))\n    print(\"\\nClassification Report:\\n\", classification_report(final_targets, final_outputs, target_names=target_list))\n\n    print(\"\\n\\n\")\n    return f1, average_loss\n## Training & Evaluation\n# recording starting time\nstart = time.time()\n\nhistory = defaultdict(list)\nbest_f1 = 0.0\n\nfor epoch in range(1, EPOCHS+1):\n    print(f'Epoch {epoch}/{EPOCHS}')\n    model, train_acc, train_loss = train_model(train_data_loader, model, optimizer)\n    val_f1, val_loss = eval_model(val_data_loader, model)\n\n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_f1'].append(val_f1)\n    history['val_loss'].append(val_loss)\n    # save the best model\n    if val_f1 > best_f1:\n        torch.save(model.state_dict(), \"ohsumed_XLNET_8_MLTC_model_state.bin\")\n        best_f1 = val_f1\n\n# recording end time\nend = time.time()\nprint(f\"Total training time: {end - start} seconds\")\n## Testing\n# Loading pretrained model (best model)\nprint(\"\\n\\nTesting\\n\\n\")\nmodel = XLNETBase(num_classes=len(target_list))\nmodel.load_state_dict(torch.load(\"ohsumed_XLNET_8_MLTC_model_state.bin\"))\nmodel = model.to(device)\n\n# recording starting time\nstart = time.time()\n# Evaluate the model using the test data\neval_model(test_data_loader, model)\n# recording end time\nend = time.time()\nprint(f\"Total evaluation time: {end - start} seconds\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}