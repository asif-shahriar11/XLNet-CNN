{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9132053,"sourceType":"datasetVersion","datasetId":5513851}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install numpy\n# !pip install pandas\n# !pip install scikit-learn\n# !pip install torch\n# !pip install transformers\n\nfrom transformers import BertModel, BertTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, hamming_loss, roc_auc_score, average_precision_score\nfrom collections import defaultdict\nfrom torch.cuda.amp import autocast, GradScaler\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport time\n## Hyperparameters\nMAX_LEN = 512\nTRAIN_BATCH_SIZE = 8\nVALID_BATCH_SIZE = 8\nTEST_BATCH_SIZE = 8\nEPOCHS = 15\nLEARNING_RATE = 1e-05\nTHRESHOLD = 0.5 # threshold for the sigmoid\n## Dataset Class\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, df, tokenizer, max_len, target_list):\n        self.tokenizer = tokenizer\n        self.df = df\n        self.title = list(df['comment'])\n        self.targets = self.df[target_list].values\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.title)\n\n    def __getitem__(self, index):\n        title = str(self.title[index])\n        title = \" \".join(title.split())\n        inputs = self.tokenizer.encode_plus(\n            title,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_token_type_ids=True,\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': inputs['input_ids'].flatten(),\n            'attention_mask': inputs['attention_mask'].flatten(),\n            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n            'targets': torch.FloatTensor(self.targets[index]),\n            'title': title\n        }\n## Data\nfile = '/kaggle/input/hatespeech/hate_speech.csv'\ndf = pd.read_csv(file)\n# Split the data into train, validation, and test sets\ntemp_df, test_df = train_test_split(df, random_state=88, test_size=0.20, shuffle=True)\ntrain_df, val_df = train_test_split(temp_df, random_state=88, test_size=0.20, shuffle=True)\n\ntarget_list = list(train_df.columns[1:])\n## Tokenizer\ntokenizer = BertTokenizer.from_pretrained('GroNLP/hateBERT')\ntrain_dataset = CustomDataset(train_df, tokenizer, MAX_LEN, target_list)\nvalid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN, target_list)\ntest_dataset = CustomDataset(test_df, tokenizer, MAX_LEN, target_list)\n\n#print(train_dataset[0])\n## Data Loader\ntrain_data_loader = torch.utils.data.DataLoader(train_dataset,\n    batch_size=TRAIN_BATCH_SIZE,\n    shuffle=True,\n    num_workers=0\n)\n\nval_data_loader = torch.utils.data.DataLoader(valid_dataset,\n    batch_size=VALID_BATCH_SIZE,\n    shuffle=False,\n    num_workers=0\n)\n\ntest_data_loader = torch.utils.data.DataLoader(test_dataset,\n    batch_size=TEST_BATCH_SIZE,\n    shuffle=False,\n    num_workers=0\n)\n## Device\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndevice\n## Model\nclass HateBERTBase(nn.Module):\n    def __init__(self, num_classes):\n        super(HateBERTBase, self).__init__()\n        self.bert = BertModel.from_pretrained('GroNLP/hateBERT')\n        self.drop = nn.Dropout(0.3)\n        self.out = nn.Linear(768, num_classes)  # Assuming 5 classes for classification\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        # Get the full output\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n\n        # The last hidden state is the first element of the output\n        last_hidden_state = outputs[0]\n\n        # Pooling operation if needed, for example, using the [CLS] token's embedding\n        # Assuming [CLS] is the first token, similar to BERT. Adjust as needed.\n        pooled_output = last_hidden_state[:, 0]\n\n        output = self.drop(pooled_output)\n        return self.out(output)\n## Setting the model\nmodel = HateBERTBase(num_classes=len(target_list))\nmodel.to(device)\n## Loss & Optimizer\ndef loss_fn(outputs, targets):\n    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n\n# define the optimizer\noptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n## Training function\ndef train_model(training_loader, model, optimizer, accumulation_steps=4):\n    losses = []\n    correct_predictions = 0\n    num_samples = 0\n    total_batches = len(training_loader)\n\n    # Set model to training mode (activate dropout, batch norm)\n    model.train()\n\n    # Mixed precision\n    scaler = GradScaler()\n\n    for batch_idx, data in enumerate(training_loader):\n        ids = data['input_ids'].to(device, dtype=torch.long)\n        mask = data['attention_mask'].to(device, dtype=torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n        targets = data['targets'].to(device, dtype=torch.float)\n\n        # Forward pass with mixed precision\n        with autocast():\n            outputs = model(ids, mask, token_type_ids)  # (batch, predict) = (8, 8)\n            loss = loss_fn(outputs, targets)\n            losses.append(loss.item())\n\n        # Training accuracy, apply sigmoid, round (apply threshold 0.5)\n        outputs = torch.sigmoid(outputs).cpu().detach().numpy().round()\n        targets = targets.cpu().detach().numpy()\n        correct_predictions += np.sum(outputs == targets)\n        num_samples += targets.size  # Total number of elements in the 2D array\n\n        # Backward pass with gradient accumulation\n        loss = loss / accumulation_steps  # Normalize loss to account for accumulation\n        scaler.scale(loss).backward()\n\n        # Gradient clipping\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        # Step optimizer every accumulation_steps\n        if (batch_idx + 1) % accumulation_steps == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n\n        # Clear GPU cache\n        torch.cuda.empty_cache()\n\n    # Perform the final optimizer step if not done already\n    if (batch_idx + 1) % accumulation_steps != 0:\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad()\n\n    # Returning: trained model, model accuracy, mean loss\n    return model, float(correct_predictions) / num_samples, np.mean(losses)\n## Evaluator Function\ndef eval_model(validation_loader, model, threshold=0.5, target_list=None):\n    model.eval()\n    final_targets = []\n    final_outputs = []\n    final_probs = []\n    losses = []\n\n    # Mixed precision\n    scaler = torch.cuda.amp.GradScaler()\n\n    with torch.no_grad():\n        for data in validation_loader:\n            ids = data['input_ids'].to(device, dtype=torch.long)\n            mask = data['attention_mask'].to(device, dtype=torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n            targets = data['targets'].to(device, dtype=torch.float)\n\n            # Mixed precision forward pass\n            with torch.cuda.amp.autocast():\n                outputs = model(ids, mask, token_type_ids)\n                loss = loss_fn(outputs, targets)\n                losses.append(loss.item())\n\n                probs = torch.sigmoid(outputs).cpu().detach().numpy()\n                targets = targets.cpu().detach().numpy()\n                final_outputs.extend(probs >= threshold)\n                final_probs.extend(probs)\n                final_targets.extend(targets)\n\n            # Clear GPU cache\n            torch.cuda.empty_cache()\n\n    final_outputs = np.array(final_outputs) >= threshold\n    final_probs = np.array(final_probs)\n    final_targets = np.array(final_targets)\n\n    # Calculating metrics\n    acc = accuracy_score(final_targets, final_outputs)\n    f1 = f1_score(final_targets, final_outputs, average='weighted')  # Consider using 'macro' or 'weighted' based on your problem\n    precision = precision_score(final_targets, final_outputs, average='weighted')\n    recall = recall_score(final_targets, final_outputs, average='weighted')\n    hamming = hamming_loss(final_targets, final_outputs)\n\n    auc_roc = roc_auc_score(final_targets, final_probs, average='weighted', multi_class='ovr')\n    aupr = average_precision_score(final_targets, final_probs, average='weighted')\n\n    average_loss = np.mean(losses)\n\n    print(f\"Accuracy: {acc}\")\n    print(f\"F1 Score: {f1}\")\n    print(f\"Precision: {precision}\")\n    print(f\"Recall: {recall}\")\n    print(f\"Hamming Loss: {hamming}\")\n    print(f\"Average Loss: {average_loss}\")\n    print(f\"AUC-ROC: {auc_roc}\")\n    print(f\"AUPR: {aupr}\")\n    # Detailed classification report\n#     if target_list:\n#         print(\"\\nClassification Report:\\n\", classification_report(final_targets, final_outputs, target_names=target_list))\n    print(\"\\nClassification Report:\\n\", classification_report(final_targets, final_outputs, target_names=target_list))\n\n    print(\"\\n\\n\")\n    return f1, average_loss\n## Training & Evaluation\n# recording starting time\nstart = time.time()\n\nhistory = defaultdict(list)\nbest_f1 = 0.0\n\nfor epoch in range(1, EPOCHS+1):\n    print(f'Epoch {epoch}/{EPOCHS}')\n    model, train_acc, train_loss = train_model(train_data_loader, model, optimizer)\n    val_f1, val_loss = eval_model(val_data_loader, model)\n\n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_f1'].append(val_f1)\n    history['val_loss'].append(val_loss)\n    # save the best model\n    if val_f1 > best_f1:\n        torch.save(model.state_dict(), \"hate_HATEBERT_8_MLTC_model_state.bin\")\n        best_f1 = val_f1\n\n# recording end time\nend = time.time()\nprint(f\"Total training and validation time: {end - start} seconds\")\n## Testing\n# Loading pretrained model (best model)\nprint(\"\\n\\nTesting\\n\\n\")\nmodel = HateBERTBase(num_classes=len(target_list))\nmodel.load_state_dict(torch.load(\"hate_HATEBERT_8_MLTC_model_state.bin\"))\nmodel = model.to(device)\n\n# recording starting time\nstart = time.time()\n# Evaluate the model using the test data\neval_model(test_data_loader, model)\n# recording end time\nend = time.time()\nprint(f\"Test-set evaluation time: {end - start} seconds\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-18T15:50:31.339553Z","iopub.execute_input":"2024-08-18T15:50:31.339890Z","iopub.status.idle":"2024-08-18T18:05:37.457275Z","shell.execute_reply.started":"2024-08-18T15:50:31.339863Z","shell.execute_reply":"2024-08-18T18:05:37.456255Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/151 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1986ed9ce9fd4014bcaee031b7e81c0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17befe29035a49fba47a0da6c4754874"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5799fce2695a401e8167c1789e2ae4f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cba4cc3226394491ae4284aefc670624"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49e77f81dce947988c199f2a063a329d"}},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.1103202846975089\nF1 Score: 0.13515891679275666\nPrecision: 0.37700194067478165\nRecall: 0.10578911095796002\nHamming Loss: 0.29008642602948653\nAverage Loss: 0.5715423410500937\nAUC-ROC: 0.588700260151356\nAUPR: 0.41845819191828515\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.54      0.32      0.40       918\n           1       0.86      0.02      0.03       703\n           2       0.00      0.00      0.00       473\n           3       0.00      0.00      0.00       316\n           4       0.00      0.00      0.00       492\n\n   micro avg       0.54      0.11      0.18      2902\n   macro avg       0.28      0.07      0.09      2902\nweighted avg       0.38      0.11      0.14      2902\n samples avg       0.15      0.11      0.12      2902\n\n\n\n\nEpoch 2/15\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.15353329944077276\nF1 Score: 0.21458347351008916\nPrecision: 0.4193083309712094\nRecall: 0.17195037904893176\nHamming Loss: 0.2677173360447382\nAverage Loss: 0.5495722381080069\nAUC-ROC: 0.6883279227868775\nAUPR: 0.5291988330448225\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.66      0.47      0.55       918\n           1       0.86      0.09      0.16       703\n           2       0.00      0.00      0.00       473\n           3       0.00      0.00      0.00       316\n           4       0.00      0.00      0.00       492\n\n   micro avg       0.68      0.17      0.27      2902\n   macro avg       0.31      0.11      0.14      2902\nweighted avg       0.42      0.17      0.21      2902\n samples avg       0.24      0.17      0.19      2902\n\n\n\n\nEpoch 3/15\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.2658871377732588\nF1 Score: 0.3913265551368573\nPrecision: 0.4096742801272737\nRecall: 0.37801516195727086\nHamming Loss: 0.22562277580071174\nAverage Loss: 0.5147862075790157\nAUC-ROC: 0.7605012378432233\nAUPR: 0.6305700174404876\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.69      0.71      0.70       918\n           1       0.80      0.63      0.70       703\n           2       0.00      0.00      0.00       473\n           3       0.00      0.00      0.00       316\n           4       0.00      0.00      0.00       492\n\n   micro avg       0.73      0.38      0.50      2902\n   macro avg       0.30      0.27      0.28      2902\nweighted avg       0.41      0.38      0.39      2902\n samples avg       0.48      0.39      0.41      2902\n\n\n\n\nEpoch 4/15\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.30096593797661414\nF1 Score: 0.4260631860200604\nPrecision: 0.6978080443609344\nRecall: 0.38456237077877325\nHamming Loss: 0.20823589222165734\nAverage Loss: 0.47109228990426877\nAUC-ROC: 0.7983801976508933\nAUPR: 0.6947691961188979\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.79      0.67      0.73       918\n           1       0.83      0.68      0.75       703\n           2       0.84      0.03      0.07       473\n           3       1.00      0.02      0.04       316\n           4       0.00      0.00      0.00       492\n\n   micro avg       0.81      0.38      0.52      2902\n   macro avg       0.69      0.28      0.32      2902\nweighted avg       0.70      0.38      0.43      2902\n samples avg       0.51      0.40      0.43      2902\n\n\n\n\nEpoch 5/15\nAccuracy: 0.3431621759023894\nF1 Score: 0.496886280423716\nPrecision: 0.8656315484720473\nRecall: 0.42763611302549964\nHamming Loss: 0.1925775292323335\nAverage Loss: 0.43858414113036986\nAUC-ROC: 0.8235095826417531\nAUPR: 0.7312037245048191\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.85      0.66      0.74       918\n           1       0.84      0.72      0.78       703\n           2       0.73      0.17      0.28       473\n           3       0.96      0.14      0.25       316\n           4       1.00      0.00      0.00       492\n\n   micro avg       0.84      0.43      0.57      2902\n   macro avg       0.88      0.34      0.41      2902\nweighted avg       0.87      0.43      0.50      2902\n samples avg       0.58      0.45      0.49      2902\n\n\n\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/15\nAccuracy: 0.3934926283680732\nF1 Score: 0.5626057811829392\nPrecision: 0.8589226540573197\nRecall: 0.4968986905582357\nHamming Loss: 0.17712252160650738\nAverage Loss: 0.4100248252109783\nAUC-ROC: 0.8414744883498742\nAUPR: 0.7549707443838691\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.84      0.71      0.77       918\n           1       0.83      0.78      0.80       703\n           2       0.72      0.25      0.37       473\n           3       0.95      0.39      0.55       316\n           4       1.00      0.01      0.02       492\n\n   micro avg       0.84      0.50      0.62      2902\n   macro avg       0.87      0.43      0.50      2902\nweighted avg       0.86      0.50      0.56      2902\n samples avg       0.67      0.53      0.57      2902\n\n\n\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/15\nAccuracy: 0.42857142857142855\nF1 Score: 0.6068656173419458\nPrecision: 0.8485833835966656\nRecall: 0.5427291523087526\nHamming Loss: 0.1638027452974072\nAverage Loss: 0.38667355338490106\nAUC-ROC: 0.8548938952127905\nAUPR: 0.7738385810610989\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.83      0.75      0.79       918\n           1       0.88      0.79      0.83       703\n           2       0.73      0.29      0.42       473\n           3       0.93      0.55      0.69       316\n           4       0.90      0.04      0.07       492\n\n   micro avg       0.85      0.54      0.66      2902\n   macro avg       0.85      0.49      0.56      2902\nweighted avg       0.85      0.54      0.61      2902\n samples avg       0.72      0.57      0.61      2902\n\n\n\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/15\nAccuracy: 0.45348246059989833\nF1 Score: 0.6380473911192813\nPrecision: 0.8524541006906582\nRecall: 0.5696071674707098\nHamming Loss: 0.1541433655312659\nAverage Loss: 0.36899811386819775\nAUC-ROC: 0.8630863339128839\nAUPR: 0.7861934679214775\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.86      0.73      0.79       918\n           1       0.88      0.83      0.85       703\n           2       0.74      0.32      0.45       473\n           3       0.91      0.67      0.77       316\n           4       0.87      0.08      0.15       492\n\n   micro avg       0.86      0.57      0.69      2902\n   macro avg       0.85      0.52      0.60      2902\nweighted avg       0.85      0.57      0.64      2902\n samples avg       0.75      0.60      0.64      2902\n\n\n\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/15\nAccuracy: 0.4753431621759024\nF1 Score: 0.6690481847189657\nPrecision: 0.8468476454415115\nRecall: 0.6016540317022743\nHamming Loss: 0.14560244026436198\nAverage Loss: 0.35590645828382755\nAUC-ROC: 0.8689106229742573\nAUPR: 0.7950762905123546\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.85      0.74      0.79       918\n           1       0.90      0.84      0.87       703\n           2       0.75      0.40      0.52       473\n           3       0.92      0.75      0.82       316\n           4       0.80      0.11      0.20       492\n\n   micro avg       0.86      0.60      0.71      2902\n   macro avg       0.85      0.57      0.64      2902\nweighted avg       0.85      0.60      0.67      2902\n samples avg       0.77      0.63      0.67      2902\n\n\n\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/15\nAccuracy: 0.4814438230808338\nF1 Score: 0.6738194467869623\nPrecision: 0.8413108841929243\nRecall: 0.6061337008959339\nHamming Loss: 0.14285714285714285\nAverage Loss: 0.34644212234553284\nAUC-ROC: 0.8734148032642504\nAUPR: 0.8017349715379768\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.88      0.73      0.80       918\n           1       0.89      0.87      0.88       703\n           2       0.79      0.32      0.45       473\n           3       0.91      0.81      0.86       316\n           4       0.70      0.15      0.24       492\n\n   micro avg       0.87      0.61      0.71      2902\n   macro avg       0.84      0.57      0.65      2902\nweighted avg       0.84      0.61      0.67      2902\n samples avg       0.78      0.64      0.68      2902\n\n\n\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/15\nAccuracy: 0.4961870869344179\nF1 Score: 0.6920961906432189\nPrecision: 0.8266603523198525\nRecall: 0.642315644383184\nHamming Loss: 0.13990849008642603\nAverage Loss: 0.3405074958151918\nAUC-ROC: 0.8774464785896711\nAUPR: 0.8061216983935605\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.84      0.78      0.81       918\n           1       0.88      0.89      0.88       703\n           2       0.75      0.43      0.54       473\n           3       0.92      0.80      0.86       316\n           4       0.75      0.14      0.24       492\n\n   micro avg       0.85      0.64      0.73      2902\n   macro avg       0.83      0.61      0.67      2902\nweighted avg       0.83      0.64      0.69      2902\n samples avg       0.79      0.67      0.70      2902\n\n\n\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/15\nAccuracy: 0.5038129130655821\nF1 Score: 0.7045519134997775\nPrecision: 0.8302195077094605\nRecall: 0.6523087525844246\nHamming Loss: 0.13655312658871377\nAverage Loss: 0.33408652645785636\nAUC-ROC: 0.8806655977993373\nAUPR: 0.8107920322206694\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.83      0.79      0.81       918\n           1       0.90      0.86      0.88       703\n           2       0.79      0.40      0.53       473\n           3       0.92      0.87      0.89       316\n           4       0.73      0.20      0.31       492\n\n   micro avg       0.85      0.65      0.74      2902\n   macro avg       0.83      0.62      0.68      2902\nweighted avg       0.83      0.65      0.70      2902\n samples avg       0.81      0.68      0.71      2902\n\n\n\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/15\nAccuracy: 0.5088967971530249\nF1 Score: 0.7144004371722011\nPrecision: 0.8490662796867755\nRecall: 0.6498966230186078\nHamming Loss: 0.1318759532282664\nAverage Loss: 0.3279996446477688\nAUC-ROC: 0.8828194769776774\nAUPR: 0.8150951410386348\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.87      0.76      0.81       918\n           1       0.90      0.87      0.89       703\n           2       0.80      0.39      0.52       473\n           3       0.92      0.87      0.89       316\n           4       0.74      0.23      0.35       492\n\n   micro avg       0.87      0.65      0.74      2902\n   macro avg       0.85      0.62      0.69      2902\nweighted avg       0.85      0.65      0.71      2902\n samples avg       0.82      0.68      0.72      2902\n\n\n\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/15\nAccuracy: 0.5241484494153533\nF1 Score: 0.7240100124670197\nPrecision: 0.8334795806989973\nRecall: 0.6753962784286699\nHamming Loss: 0.13035078800203356\nAverage Loss: 0.3260298543041799\nAUC-ROC: 0.8850151517310225\nAUPR: 0.8176606267782325\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.85      0.79      0.82       918\n           1       0.88      0.91      0.90       703\n           2       0.76      0.45      0.56       473\n           3       0.92      0.87      0.90       316\n           4       0.75      0.23      0.35       492\n\n   micro avg       0.85      0.68      0.75      2902\n   macro avg       0.83      0.65      0.70      2902\nweighted avg       0.83      0.68      0.72      2902\n samples avg       0.82      0.70      0.73      2902\n\n\n\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/15\nAccuracy: 0.5287239450940519\nF1 Score: 0.7329884547762499\nPrecision: 0.8409441388879961\nRecall: 0.6788421778084079\nHamming Loss: 0.1275038129130656\nAverage Loss: 0.3215156012494874\nAUC-ROC: 0.8868658512382673\nAUPR: 0.8204472402782581\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.87      0.77      0.82       918\n           1       0.89      0.91      0.90       703\n           2       0.77      0.44      0.56       473\n           3       0.92      0.89      0.90       316\n           4       0.74      0.27      0.40       492\n\n   micro avg       0.86      0.68      0.76      2902\n   macro avg       0.84      0.66      0.71      2902\nweighted avg       0.84      0.68      0.73      2902\n samples avg       0.82      0.70      0.73      2902\n\n\n\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Total training and validation time: 8043.953922510147 seconds\n\n\nTesting\n\n\nAccuracy: 0.5282635217568117\nF1 Score: 0.7377497321769548\nPrecision: 0.8238034141483837\nRecall: 0.6885199774901519\nHamming Loss: 0.12566083773891826\nAverage Loss: 0.3180476566171878\nAUC-ROC: 0.8868053476428877\nAUPR: 0.8092640336547569\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.87      0.80      0.84      1149\n           1       0.89      0.89      0.89       861\n           2       0.75      0.45      0.56       579\n           3       0.90      0.88      0.89       386\n           4       0.65      0.28      0.39       579\n\n   micro avg       0.85      0.69      0.76      3554\n   macro avg       0.81      0.66      0.71      3554\nweighted avg       0.82      0.69      0.74      3554\n samples avg       0.81      0.70      0.73      3554\n\n\n\n\nTest-set evaluation time: 46.0760293006897 seconds\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":1}]}