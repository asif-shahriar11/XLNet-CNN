{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-18T15:50:31.339890Z",
     "iopub.status.busy": "2024-08-18T15:50:31.339553Z",
     "iopub.status.idle": "2024-08-18T18:05:37.457275Z",
     "shell.execute_reply": "2024-08-18T18:05:37.456255Z",
     "shell.execute_reply.started": "2024-08-18T15:50:31.339863Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1986ed9ce9fd4014bcaee031b7e81c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/151 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17befe29035a49fba47a0da6c4754874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5799fce2695a401e8167c1789e2ae4f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba4cc3226394491ae4284aefc670624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e77f81dce947988c199f2a063a329d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1103202846975089\n",
      "F1 Score: 0.13515891679275666\n",
      "Precision: 0.37700194067478165\n",
      "Recall: 0.10578911095796002\n",
      "Hamming Loss: 0.29008642602948653\n",
      "Average Loss: 0.5715423410500937\n",
      "AUC-ROC: 0.588700260151356\n",
      "AUPR: 0.41845819191828515\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.32      0.40       918\n",
      "           1       0.86      0.02      0.03       703\n",
      "           2       0.00      0.00      0.00       473\n",
      "           3       0.00      0.00      0.00       316\n",
      "           4       0.00      0.00      0.00       492\n",
      "\n",
      "   micro avg       0.54      0.11      0.18      2902\n",
      "   macro avg       0.28      0.07      0.09      2902\n",
      "weighted avg       0.38      0.11      0.14      2902\n",
      " samples avg       0.15      0.11      0.12      2902\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.15353329944077276\n",
      "F1 Score: 0.21458347351008916\n",
      "Precision: 0.4193083309712094\n",
      "Recall: 0.17195037904893176\n",
      "Hamming Loss: 0.2677173360447382\n",
      "Average Loss: 0.5495722381080069\n",
      "AUC-ROC: 0.6883279227868775\n",
      "AUPR: 0.5291988330448225\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.47      0.55       918\n",
      "           1       0.86      0.09      0.16       703\n",
      "           2       0.00      0.00      0.00       473\n",
      "           3       0.00      0.00      0.00       316\n",
      "           4       0.00      0.00      0.00       492\n",
      "\n",
      "   micro avg       0.68      0.17      0.27      2902\n",
      "   macro avg       0.31      0.11      0.14      2902\n",
      "weighted avg       0.42      0.17      0.21      2902\n",
      " samples avg       0.24      0.17      0.19      2902\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2658871377732588\n",
      "F1 Score: 0.3913265551368573\n",
      "Precision: 0.4096742801272737\n",
      "Recall: 0.37801516195727086\n",
      "Hamming Loss: 0.22562277580071174\n",
      "Average Loss: 0.5147862075790157\n",
      "AUC-ROC: 0.7605012378432233\n",
      "AUPR: 0.6305700174404876\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.71      0.70       918\n",
      "           1       0.80      0.63      0.70       703\n",
      "           2       0.00      0.00      0.00       473\n",
      "           3       0.00      0.00      0.00       316\n",
      "           4       0.00      0.00      0.00       492\n",
      "\n",
      "   micro avg       0.73      0.38      0.50      2902\n",
      "   macro avg       0.30      0.27      0.28      2902\n",
      "weighted avg       0.41      0.38      0.39      2902\n",
      " samples avg       0.48      0.39      0.41      2902\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.30096593797661414\n",
      "F1 Score: 0.4260631860200604\n",
      "Precision: 0.6978080443609344\n",
      "Recall: 0.38456237077877325\n",
      "Hamming Loss: 0.20823589222165734\n",
      "Average Loss: 0.47109228990426877\n",
      "AUC-ROC: 0.7983801976508933\n",
      "AUPR: 0.6947691961188979\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.67      0.73       918\n",
      "           1       0.83      0.68      0.75       703\n",
      "           2       0.84      0.03      0.07       473\n",
      "           3       1.00      0.02      0.04       316\n",
      "           4       0.00      0.00      0.00       492\n",
      "\n",
      "   micro avg       0.81      0.38      0.52      2902\n",
      "   macro avg       0.69      0.28      0.32      2902\n",
      "weighted avg       0.70      0.38      0.43      2902\n",
      " samples avg       0.51      0.40      0.43      2902\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 5/15\n",
      "Accuracy: 0.3431621759023894\n",
      "F1 Score: 0.496886280423716\n",
      "Precision: 0.8656315484720473\n",
      "Recall: 0.42763611302549964\n",
      "Hamming Loss: 0.1925775292323335\n",
      "Average Loss: 0.43858414113036986\n",
      "AUC-ROC: 0.8235095826417531\n",
      "AUPR: 0.7312037245048191\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.66      0.74       918\n",
      "           1       0.84      0.72      0.78       703\n",
      "           2       0.73      0.17      0.28       473\n",
      "           3       0.96      0.14      0.25       316\n",
      "           4       1.00      0.00      0.00       492\n",
      "\n",
      "   micro avg       0.84      0.43      0.57      2902\n",
      "   macro avg       0.88      0.34      0.41      2902\n",
      "weighted avg       0.87      0.43      0.50      2902\n",
      " samples avg       0.58      0.45      0.49      2902\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15\n",
      "Accuracy: 0.3934926283680732\n",
      "F1 Score: 0.5626057811829392\n",
      "Precision: 0.8589226540573197\n",
      "Recall: 0.4968986905582357\n",
      "Hamming Loss: 0.17712252160650738\n",
      "Average Loss: 0.4100248252109783\n",
      "AUC-ROC: 0.8414744883498742\n",
      "AUPR: 0.7549707443838691\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.71      0.77       918\n",
      "           1       0.83      0.78      0.80       703\n",
      "           2       0.72      0.25      0.37       473\n",
      "           3       0.95      0.39      0.55       316\n",
      "           4       1.00      0.01      0.02       492\n",
      "\n",
      "   micro avg       0.84      0.50      0.62      2902\n",
      "   macro avg       0.87      0.43      0.50      2902\n",
      "weighted avg       0.86      0.50      0.56      2902\n",
      " samples avg       0.67      0.53      0.57      2902\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15\n",
      "Accuracy: 0.42857142857142855\n",
      "F1 Score: 0.6068656173419458\n",
      "Precision: 0.8485833835966656\n",
      "Recall: 0.5427291523087526\n",
      "Hamming Loss: 0.1638027452974072\n",
      "Average Loss: 0.38667355338490106\n",
      "AUC-ROC: 0.8548938952127905\n",
      "AUPR: 0.7738385810610989\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.75      0.79       918\n",
      "           1       0.88      0.79      0.83       703\n",
      "           2       0.73      0.29      0.42       473\n",
      "           3       0.93      0.55      0.69       316\n",
      "           4       0.90      0.04      0.07       492\n",
      "\n",
      "   micro avg       0.85      0.54      0.66      2902\n",
      "   macro avg       0.85      0.49      0.56      2902\n",
      "weighted avg       0.85      0.54      0.61      2902\n",
      " samples avg       0.72      0.57      0.61      2902\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15\n",
      "Accuracy: 0.45348246059989833\n",
      "F1 Score: 0.6380473911192813\n",
      "Precision: 0.8524541006906582\n",
      "Recall: 0.5696071674707098\n",
      "Hamming Loss: 0.1541433655312659\n",
      "Average Loss: 0.36899811386819775\n",
      "AUC-ROC: 0.8630863339128839\n",
      "AUPR: 0.7861934679214775\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.73      0.79       918\n",
      "           1       0.88      0.83      0.85       703\n",
      "           2       0.74      0.32      0.45       473\n",
      "           3       0.91      0.67      0.77       316\n",
      "           4       0.87      0.08      0.15       492\n",
      "\n",
      "   micro avg       0.86      0.57      0.69      2902\n",
      "   macro avg       0.85      0.52      0.60      2902\n",
      "weighted avg       0.85      0.57      0.64      2902\n",
      " samples avg       0.75      0.60      0.64      2902\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15\n",
      "Accuracy: 0.4753431621759024\n",
      "F1 Score: 0.6690481847189657\n",
      "Precision: 0.8468476454415115\n",
      "Recall: 0.6016540317022743\n",
      "Hamming Loss: 0.14560244026436198\n",
      "Average Loss: 0.35590645828382755\n",
      "AUC-ROC: 0.8689106229742573\n",
      "AUPR: 0.7950762905123546\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.74      0.79       918\n",
      "           1       0.90      0.84      0.87       703\n",
      "           2       0.75      0.40      0.52       473\n",
      "           3       0.92      0.75      0.82       316\n",
      "           4       0.80      0.11      0.20       492\n",
      "\n",
      "   micro avg       0.86      0.60      0.71      2902\n",
      "   macro avg       0.85      0.57      0.64      2902\n",
      "weighted avg       0.85      0.60      0.67      2902\n",
      " samples avg       0.77      0.63      0.67      2902\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15\n",
      "Accuracy: 0.4814438230808338\n",
      "F1 Score: 0.6738194467869623\n",
      "Precision: 0.8413108841929243\n",
      "Recall: 0.6061337008959339\n",
      "Hamming Loss: 0.14285714285714285\n",
      "Average Loss: 0.34644212234553284\n",
      "AUC-ROC: 0.8734148032642504\n",
      "AUPR: 0.8017349715379768\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.73      0.80       918\n",
      "           1       0.89      0.87      0.88       703\n",
      "           2       0.79      0.32      0.45       473\n",
      "           3       0.91      0.81      0.86       316\n",
      "           4       0.70      0.15      0.24       492\n",
      "\n",
      "   micro avg       0.87      0.61      0.71      2902\n",
      "   macro avg       0.84      0.57      0.65      2902\n",
      "weighted avg       0.84      0.61      0.67      2902\n",
      " samples avg       0.78      0.64      0.68      2902\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15\n",
      "Accuracy: 0.4961870869344179\n",
      "F1 Score: 0.6920961906432189\n",
      "Precision: 0.8266603523198525\n",
      "Recall: 0.642315644383184\n",
      "Hamming Loss: 0.13990849008642603\n",
      "Average Loss: 0.3405074958151918\n",
      "AUC-ROC: 0.8774464785896711\n",
      "AUPR: 0.8061216983935605\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.78      0.81       918\n",
      "           1       0.88      0.89      0.88       703\n",
      "           2       0.75      0.43      0.54       473\n",
      "           3       0.92      0.80      0.86       316\n",
      "           4       0.75      0.14      0.24       492\n",
      "\n",
      "   micro avg       0.85      0.64      0.73      2902\n",
      "   macro avg       0.83      0.61      0.67      2902\n",
      "weighted avg       0.83      0.64      0.69      2902\n",
      " samples avg       0.79      0.67      0.70      2902\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15\n",
      "Accuracy: 0.5038129130655821\n",
      "F1 Score: 0.7045519134997775\n",
      "Precision: 0.8302195077094605\n",
      "Recall: 0.6523087525844246\n",
      "Hamming Loss: 0.13655312658871377\n",
      "Average Loss: 0.33408652645785636\n",
      "AUC-ROC: 0.8806655977993373\n",
      "AUPR: 0.8107920322206694\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.79      0.81       918\n",
      "           1       0.90      0.86      0.88       703\n",
      "           2       0.79      0.40      0.53       473\n",
      "           3       0.92      0.87      0.89       316\n",
      "           4       0.73      0.20      0.31       492\n",
      "\n",
      "   micro avg       0.85      0.65      0.74      2902\n",
      "   macro avg       0.83      0.62      0.68      2902\n",
      "weighted avg       0.83      0.65      0.70      2902\n",
      " samples avg       0.81      0.68      0.71      2902\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15\n",
      "Accuracy: 0.5088967971530249\n",
      "F1 Score: 0.7144004371722011\n",
      "Precision: 0.8490662796867755\n",
      "Recall: 0.6498966230186078\n",
      "Hamming Loss: 0.1318759532282664\n",
      "Average Loss: 0.3279996446477688\n",
      "AUC-ROC: 0.8828194769776774\n",
      "AUPR: 0.8150951410386348\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.76      0.81       918\n",
      "           1       0.90      0.87      0.89       703\n",
      "           2       0.80      0.39      0.52       473\n",
      "           3       0.92      0.87      0.89       316\n",
      "           4       0.74      0.23      0.35       492\n",
      "\n",
      "   micro avg       0.87      0.65      0.74      2902\n",
      "   macro avg       0.85      0.62      0.69      2902\n",
      "weighted avg       0.85      0.65      0.71      2902\n",
      " samples avg       0.82      0.68      0.72      2902\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15\n",
      "Accuracy: 0.5241484494153533\n",
      "F1 Score: 0.7240100124670197\n",
      "Precision: 0.8334795806989973\n",
      "Recall: 0.6753962784286699\n",
      "Hamming Loss: 0.13035078800203356\n",
      "Average Loss: 0.3260298543041799\n",
      "AUC-ROC: 0.8850151517310225\n",
      "AUPR: 0.8176606267782325\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.79      0.82       918\n",
      "           1       0.88      0.91      0.90       703\n",
      "           2       0.76      0.45      0.56       473\n",
      "           3       0.92      0.87      0.90       316\n",
      "           4       0.75      0.23      0.35       492\n",
      "\n",
      "   micro avg       0.85      0.68      0.75      2902\n",
      "   macro avg       0.83      0.65      0.70      2902\n",
      "weighted avg       0.83      0.68      0.72      2902\n",
      " samples avg       0.82      0.70      0.73      2902\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15\n",
      "Accuracy: 0.5287239450940519\n",
      "F1 Score: 0.7329884547762499\n",
      "Precision: 0.8409441388879961\n",
      "Recall: 0.6788421778084079\n",
      "Hamming Loss: 0.1275038129130656\n",
      "Average Loss: 0.3215156012494874\n",
      "AUC-ROC: 0.8868658512382673\n",
      "AUPR: 0.8204472402782581\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.77      0.82       918\n",
      "           1       0.89      0.91      0.90       703\n",
      "           2       0.77      0.44      0.56       473\n",
      "           3       0.92      0.89      0.90       316\n",
      "           4       0.74      0.27      0.40       492\n",
      "\n",
      "   micro avg       0.86      0.68      0.76      2902\n",
      "   macro avg       0.84      0.66      0.71      2902\n",
      "weighted avg       0.84      0.68      0.73      2902\n",
      " samples avg       0.82      0.70      0.73      2902\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training and validation time: 8043.953922510147 seconds\n",
      "\n",
      "\n",
      "Testing\n",
      "\n",
      "\n",
      "Accuracy: 0.5282635217568117\n",
      "F1 Score: 0.7377497321769548\n",
      "Precision: 0.8238034141483837\n",
      "Recall: 0.6885199774901519\n",
      "Hamming Loss: 0.12566083773891826\n",
      "Average Loss: 0.3180476566171878\n",
      "AUC-ROC: 0.8868053476428877\n",
      "AUPR: 0.8092640336547569\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.80      0.84      1149\n",
      "           1       0.89      0.89      0.89       861\n",
      "           2       0.75      0.45      0.56       579\n",
      "           3       0.90      0.88      0.89       386\n",
      "           4       0.65      0.28      0.39       579\n",
      "\n",
      "   micro avg       0.85      0.69      0.76      3554\n",
      "   macro avg       0.81      0.66      0.71      3554\n",
      "weighted avg       0.82      0.69      0.74      3554\n",
      " samples avg       0.81      0.70      0.73      3554\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test-set evaluation time: 46.0760293006897 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install scikit-learn\n",
    "# !pip install torch\n",
    "# !pip install transformers\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, hamming_loss, roc_auc_score, average_precision_score\n",
    "from collections import defaultdict\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "## Hyperparameters\n",
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 8\n",
    "TEST_BATCH_SIZE = 8\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 1e-05\n",
    "THRESHOLD = 0.5 # threshold for the sigmoid\n",
    "## Dataset Class\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len, target_list):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.title = list(df['comment'])\n",
    "        self.targets = self.df[target_list].values\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.title[index])\n",
    "        title = \" \".join(title.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
    "            'targets': torch.FloatTensor(self.targets[index]),\n",
    "            'title': title\n",
    "        }\n",
    "    \n",
    "## Data\n",
    "file = '../data/hatespeech/hate_speech.csv'\n",
    "df = pd.read_csv(file)\n",
    "# Split the data into train, validation, and test sets\n",
    "temp_df, test_df = train_test_split(df, random_state=88, test_size=0.20, shuffle=True)\n",
    "train_df, val_df = train_test_split(temp_df, random_state=88, test_size=0.20, shuffle=True)\n",
    "\n",
    "target_list = list(train_df.columns[1:])\n",
    "\n",
    "## Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('GroNLP/hateBERT')\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN, target_list)\n",
    "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN, target_list)\n",
    "test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN, target_list)\n",
    "\n",
    "#print(train_dataset[0])\n",
    "\n",
    "## Data Loader\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "    batch_size=VALID_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "## Device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device\n",
    "\n",
    "## Model\n",
    "class HateBERTBase(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(HateBERTBase, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('GroNLP/hateBERT')\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        self.out = nn.Linear(768, num_classes) \n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # Get the full output\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        # The last hidden state is the first element of the output\n",
    "        last_hidden_state = outputs[0]\n",
    "\n",
    "        # Pooling operation if needed, for example, using the [CLS] token's embedding\n",
    "        # Assuming [CLS] is the first token, similar to BERT. Adjust as needed.\n",
    "        pooled_output = last_hidden_state[:, 0]\n",
    "\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)\n",
    "    \n",
    "\n",
    "## Setting the model\n",
    "model = HateBERTBase(num_classes=len(target_list))\n",
    "model.to(device)\n",
    "\n",
    "## Loss & Optimizer\n",
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "## Training function\n",
    "def train_model(training_loader, model, optimizer, accumulation_steps=4):\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    total_batches = len(training_loader)\n",
    "\n",
    "    # Set model to training mode (activate dropout, batch norm)\n",
    "    model.train()\n",
    "\n",
    "    # Mixed precision\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for batch_idx, data in enumerate(training_loader):\n",
    "        ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "        targets = data['targets'].to(device, dtype=torch.float)\n",
    "\n",
    "        # Forward pass with mixed precision\n",
    "        with autocast():\n",
    "            outputs = model(ids, mask, token_type_ids)  # (batch, predict) = (8, 8)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        # Training accuracy, apply sigmoid, round (apply threshold 0.5)\n",
    "        outputs = torch.sigmoid(outputs).cpu().detach().numpy().round()\n",
    "        targets = targets.cpu().detach().numpy()\n",
    "        correct_predictions += np.sum(outputs == targets)\n",
    "        num_samples += targets.size  # Total number of elements in the 2D array\n",
    "\n",
    "        # Backward pass with gradient accumulation\n",
    "        loss = loss / accumulation_steps  # Normalize loss to account for accumulation\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Step optimizer every accumulation_steps\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Clear GPU cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Perform the final optimizer step if not done already\n",
    "    if (batch_idx + 1) % accumulation_steps != 0:\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Returning: trained model, model accuracy, mean loss\n",
    "    return model, float(correct_predictions) / num_samples, np.mean(losses)\n",
    "\n",
    "\n",
    "## Evaluator Function\n",
    "def eval_model(validation_loader, model, threshold=0.5, target_list=None):\n",
    "    model.eval()\n",
    "    final_targets = []\n",
    "    final_outputs = []\n",
    "    final_probs = []\n",
    "    losses = []\n",
    "\n",
    "    # Mixed precision\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in validation_loader:\n",
    "            ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = data['targets'].to(device, dtype=torch.float)\n",
    "\n",
    "            # Mixed precision forward pass\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(ids, mask, token_type_ids)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "                probs = torch.sigmoid(outputs).cpu().detach().numpy()\n",
    "                targets = targets.cpu().detach().numpy()\n",
    "                final_outputs.extend(probs >= threshold)\n",
    "                final_probs.extend(probs)\n",
    "                final_targets.extend(targets)\n",
    "\n",
    "            # Clear GPU cache\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    final_outputs = np.array(final_outputs) >= threshold\n",
    "    final_probs = np.array(final_probs)\n",
    "    final_targets = np.array(final_targets)\n",
    "\n",
    "    # Calculating metrics\n",
    "    acc = accuracy_score(final_targets, final_outputs)\n",
    "    f1 = f1_score(final_targets, final_outputs, average='weighted')  # Consider using 'macro' or 'weighted' based on your problem\n",
    "    precision = precision_score(final_targets, final_outputs, average='weighted')\n",
    "    recall = recall_score(final_targets, final_outputs, average='weighted')\n",
    "    hamming = hamming_loss(final_targets, final_outputs)\n",
    "\n",
    "    auc_roc = roc_auc_score(final_targets, final_probs, average='weighted', multi_class='ovr')\n",
    "    aupr = average_precision_score(final_targets, final_probs, average='weighted')\n",
    "\n",
    "    average_loss = np.mean(losses)\n",
    "\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"Hamming Loss: {hamming}\")\n",
    "    print(f\"Average Loss: {average_loss}\")\n",
    "    print(f\"AUC-ROC: {auc_roc}\")\n",
    "    print(f\"AUPR: {aupr}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(final_targets, final_outputs, target_names=target_list))\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    return f1, average_loss\n",
    "\n",
    "\n",
    "## Training & Evaluation\n",
    "# recording starting time\n",
    "start = time.time()\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_f1 = 0.0\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(f'Epoch {epoch}/{EPOCHS}')\n",
    "    model, train_acc, train_loss = train_model(train_data_loader, model, optimizer)\n",
    "    val_f1, val_loss = eval_model(val_data_loader, model)\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    # save the best model\n",
    "    if val_f1 > best_f1:\n",
    "        torch.save(model.state_dict(), \"hate_HATEBERT_8_MLTC_model_state.bin\")\n",
    "        best_f1 = val_f1\n",
    "\n",
    "# recording end time\n",
    "end = time.time()\n",
    "print(f\"Total training and validation time: {end - start} seconds\")\n",
    "\n",
    "## Testing\n",
    "# Loading pretrained model (best model)\n",
    "print(\"\\n\\nTesting\\n\\n\")\n",
    "model = HateBERTBase(num_classes=len(target_list))\n",
    "model.load_state_dict(torch.load(\"hate_HATEBERT_8_MLTC_model_state.bin\"))\n",
    "model = model.to(device)\n",
    "\n",
    "# recording starting time\n",
    "start = time.time()\n",
    "# Evaluate the model using the test data\n",
    "eval_model(test_data_loader, model)\n",
    "# recording end time\n",
    "end = time.time()\n",
    "print(f\"Test-set evaluation time: {end - start} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5513851,
     "sourceId": 9132053,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
